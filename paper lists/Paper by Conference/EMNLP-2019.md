# Machine Translation Paper List (EMNLP-2019)

### Long paper (26)

| Paper                                                     | Authors                                                  | Domain             | Link                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- | -------------------------------- |
| Explicit Cross-lingual Pre-training for Unsupervised Machine Translation | *Ren Shuo, Wu Yu, Liu Shujie,  Zhou Ming and Ma, Shuai* | Unsupervised | https://www.aclweb.org/anthology/D19-1071.pdf |
| Latent Part-of-Speech Sequences for Neural Machine Translation | *Xuewen Yang, Yingru Liu, Dongliang Xie, Xin Wang, Niranjan Balasubramanian* | External Knowledge | https://www.aclweb.org/anthology/D19-1072.pdf |
| Towards Linear Time Neural Machine Translation with Capsule Networks | *Mingxuan Wang* | Neural Network Architecture | https://www.aclweb.org/anthology/D19-1074.pdf |
| Iterative Dual Domain Adaptation for Neural Machine Translation | *Jiali Zeng, Yang Liu, jinsong su, yubing Ge, Yaojie Lu, Yongjing Yin, jiebo luo* | Domain Adaptation | https://www.aclweb.org/anthology/D19-1078.pdf |
| Multi-agent Learning for Neural Machine Translation | *tianchi bi, hao xiong, Zhongjun He, Hua Wu, Haifeng Wang* | Ensemble Training | https://www.aclweb.org/anthology/D19-1079.pdf |
| Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages | *Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, Hermann Ney* | Pre-training | https://www.aclweb.org/anthology/D19-1080.pdf |
| Context-Aware Monolingual Repair for Neural Machine Translation | *Elena Voita, Rico Sennrich, Ivan Titov* | Document Level | https://www.aclweb.org/anthology/D19-1081.pdf |
| Multi-Granularity Self-Attention for Neural Machine Translation | *Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu* | Neural Network Architecture | https://www.aclweb.org/anthology/D19-1082.pdf |
| Dynamic Past and Future for Neural Machine Translation | *Zaixiang Zheng, Shujian Huang, Zhaopeng Tu, XIN-YU DAI, Jiajun CHEN* | Coverage | https://www.aclweb.org/anthology/D19-1086.pdf |
| Towards Understanding Neural Machine Translation with Word Importance | *Shilin He, Zhaopeng Tu, Xing Wang, Longyue Wang, Michael Lyu, Shuming Shi* | Interpretability | https://www.aclweb.org/anthology/D19-1088.pdf |
| Multilingual Neural Machine Translation with Language Clustering | *Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao QIN, Tie-Yan Liu* | Multilingual | https://www.aclweb.org/anthology/D19-1089.pdf |
| Simple, Scalable Adaptation for Neural Machine Translation | *Ankur Bapna, Orhan Firat* | Fine Tuning | https://www.aclweb.org/anthology/D19-1165.pdf |
| Controlling Text Complexity in Neural Machine Translation | *Sweta Agrawal, Marine Carpuat* | Text Complexity | https://www.aclweb.org/anthology/D19-1166.pdf |
| Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation | *Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou* | Document Level | https://www.aclweb.org/anthology/D19-1168.pdf |
| Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite | *Prathyusha Jwalapuram, Shafiq Joty, Irina Temnikova, Preslav Nakov* | Evaluation Measure | https://www.aclweb.org/anthology/D19-1294.pdf |
| Exploiting Monolingual Data at Scale for Neural Machine Translation | *Lijun Wu, Yiren Wang, Yingce Xia, Tao QIN, Jianhuang Lai, Tie-Yan Liu* | Data Augmentation | https://www.aclweb.org/anthology/D19-1430.pdf |
| Machine Translation With Weakly Paired Documents             | *Lijun Wu, Jinhua Zhu, Di He, Fei Gao, Tao QIN, Jianhuang Lai, Tie-Yan Liu* | Data Augmentation       | https://www.aclweb.org/anthology/D19-1446.pdf |
| Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel | *Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, Ruslan Salakhutdinov* | Neural Network Architecture | https://www.aclweb.org/anthology/D19-1443.pdf |
| Attention is not not Explanation | *Sarah Wiegreffe, Yuval Pinter* | Interpretability | https://www.aclweb.org/anthology/D19-1002.pdf |
| The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali–English and Sinhala–English | *Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, Marc’Aurelio Ranzato* | corpus                                    | https://www.aclweb.org/anthology/D19-1632.pdf |
| The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives | *Elena Voita, Rico Sennrich, Ivan Titov*                     | Analysis                                  | https://www.aclweb.org/anthology/D19-1448.pdf |
| Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention | *Biao Zhang, Ivan Titov, Rico Sennrich*                      | Neural Network Architecture/Deep    model | https://www.aclweb.org/anthology/D19-1083.pdf |
| Tree Transformer: Integrating Tree Structures into Self-Attention | *Yaushian Wang, Hung-Yi Lee, Yun-Nung Chen*                  | Neural Network Architecture               | https://www.aclweb.org/anthology/D19-1098.pdf |
| Adaptively Sparse Transformers                               | *Gonçalo M. Correia, Vlad Niculae, André F. T. Martins*      | Neural Network Architecture               | https://www.aclweb.org/anthology/D19-1223.pdf |
| Jointly Learning to Align and Translate with Transformer Models | *Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, Matthias Paulik* | Neural Network Architecture               | https://www.aclweb.org/anthology/D19-1453.pdf |
| LXMERT: Learning Cross-Modality Encoder Representations from Transformers | *Hao Tan, Mohit Bansal*                                      | Pre-training                              | https://www.aclweb.org/anthology/D19-1514.pdf |

### Short papers (12)

| Paper                                                     | Authors                                                  | Domain             | Link                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- | -------------------------------- |
| Encoders Help You Disambiguate Word Senses in Neural Machine Translation | *Gongbo Tang, Rico Sennrich, Joakim Nivre* | Interpretability | https://www.aclweb.org/anthology/D19-1149.pdf |
| Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings | *Zi-Yi Dou, Junjie Hu, Antonios Anastasopoulos, Graham Neubig* | Domain Adaptation | https://www.aclweb.org/anthology/D19-1147.pdf |
| Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation | *Raj Dabre, Atsushi Fujita, Chenhui Chu* | Multilingual / Low Resource | https://www.aclweb.org/anthology/D19-1146.pdf |
| Handling Syntactic Divergence in Low-resource Machine Translation | *Chunting Zhou, Xuezhe Ma, Junjie Hu, Graham Neubig* | Low Resource | https://www.aclweb.org/anthology/D19-1143.pdf |
| HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation | *Brian Thompson, Rebecca Knowles, Xuan Zhang, Huda Khayrallah, Kevin Duh, Philipp Koehn* | External Knowledge | https://www.aclweb.org/anthology/D19-1142.pdf |
| Machine Translation for Machines: the Sentiment Classification Use Case | *amirhossein tebbifakhr, Luisa Bentivogli, Matteo Negri, Marco Turchi* | Training | https://www.aclweb.org/anthology/D19-1140.pdf |
| Recurrent Positional Embedding for Neural Machine Translation | *Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita* | Neural Network Architecture | https://www.aclweb.org/anthology/D19-1139.pdf |
| Self-Attention with Structural Position Representations | *Xing Wang, Zhaopeng Tu, Longyue Wang, Shuming Shi* | Neural Network Architecture | https://www.aclweb.org/anthology/D19-1145.pdf |
| Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons | *Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu* | Neural Network Architecture | https://www.aclweb.org/anthology/D19-1135.pdf |
| Hint-Based Training for Non-Autoregressive Machine Translation | *Zhuohan Li, Zi Lin, Di He, Fei Tian, Tao QIN, Liwei WANG, Tie-Yan Liu* | Non-AutoRegressive Translation | https://www.aclweb.org/anthology/D19-1573.pdf |
| Simple and Effective Noisy Channel Modeling for Neural Machine Translation | *Kyra Yee, Yann Dauphin, Michael Auli*                       | Neural Network Architecture    | https://www.aclweb.org/anthology/D19-1571.pdf |
| Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization | *Guanlin Li, Lemao Liu, Guoping Huang, Conghui Zhu, Tiejun Zhao* | Data Augmentation              | https://www.aclweb.org/anthology/D19-1570.pdf |